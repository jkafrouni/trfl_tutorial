{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ln281QTUyydR"
   },
   "source": [
    "# Intro to TRFL and OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URliwJ2uyydW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For google colab display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "PQCF6SFxz3UN",
    "outputId": "5bf8849a-8d18-48ca-f561-3125dd1964b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1007'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1007'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
    "!pip install pyvirtualdisplay > /dev/null 2>&1\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G2MXAs4ayydg"
   },
   "source": [
    "## Part 1: Intro to OpenAI Gym with Tabular Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FiB9za8Ayydj"
   },
   "source": [
    "### 1.1 What is Gym, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0pkpstg-yydm"
   },
   "source": [
    "*\"Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Pinball.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kj3WMmKpyydq"
   },
   "source": [
    "Gym only contains **environments** that you can interact with; it does not contain any Reinforcement Learning algorithms (or other techniques to write agents)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1-34UDDPyyds"
   },
   "source": [
    "People use Gym because:\n",
    "- It's (very) **easy to set up**\n",
    "- It provides a variety of **diverse environments**, some are easy benchmarks, some are hard (atari games...)\n",
    "- It allows fair **comparisons**: researcher can benchmark their techniques on known environments used by everyone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Li_zQWbeyydv"
   },
   "source": [
    "### 1.2 Installing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "qoEL8x4yyydw",
    "outputId": "54c31365-af55-4eb4-c882-218627013b4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.8)\n",
      "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.18.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (0.19.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.11.0)\n",
      "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.14.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2018.10.15)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.22)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym\n",
    "# !pip install gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_l83DODZyyd3"
   },
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nddpy_nbyyd_"
   },
   "source": [
    "### 1.3 Overview of the environments, FrozenLake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U96ArLCmyyeB"
   },
   "source": [
    "#### 1.3.1 Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RglsasuKyyeD"
   },
   "source": [
    "**Listing environments:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Pw5eaAXyyeG"
   },
   "source": [
    "Gym contains a lot of environments, all of which are listed in the env registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "QelKdW7KyyeH",
    "outputId": "60b17bfe-d72e-4a83-f49b-bdb2eaee4b24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EnvSpec(Copy-v0),\n",
      " EnvSpec(RepeatCopy-v0),\n",
      " EnvSpec(ReversedAddition-v0),\n",
      " EnvSpec(ReversedAddition3-v0),\n",
      " EnvSpec(DuplicatedInput-v0),\n",
      " EnvSpec(Reverse-v0),\n",
      " EnvSpec(CartPole-v0),\n",
      " EnvSpec(CartPole-v1),\n",
      " EnvSpec(MountainCar-v0),\n",
      " EnvSpec(MountainCarContinuous-v0),\n",
      " EnvSpec(Pendulum-v0),\n",
      " EnvSpec(Acrobot-v1),\n",
      " EnvSpec(LunarLander-v2),\n",
      " EnvSpec(LunarLanderContinuous-v2),\n",
      " EnvSpec(BipedalWalker-v2),\n",
      " EnvSpec(BipedalWalkerHardcore-v2),\n",
      " EnvSpec(CarRacing-v0),\n",
      " EnvSpec(Blackjack-v0),\n",
      " EnvSpec(KellyCoinflip-v0),\n",
      " EnvSpec(KellyCoinflipGeneralized-v0)]\n"
     ]
    }
   ],
   "source": [
    "pprint(list(gym.envs.registry.all())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WgzBHBHYyyeT",
    "outputId": "072c957c-2e96-4b8f-f99f-1e8333ae05ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "797"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(gym.envs.registry.all()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dBrVM23myyeb"
   },
   "source": [
    "Some environments are shown on gym's homepage: https://gym.openai.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B-nrmcxWyyec"
   },
   "source": [
    "**Loading an environment:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "w00TMTaEyyef",
    "outputId": "7be1adc7-f053-4563-809a-2233cfad3f8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v_4SoiTHyyel"
   },
   "source": [
    "**Rendering:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ur3M-omYyyen"
   },
   "source": [
    "On a local machine, the following code would open up a window with the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jnCkPa16yyer"
   },
   "outputs": [],
   "source": [
    "# env.render()\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d9MZAZlFyye1"
   },
   "source": [
    "Here's a trick to plot the current state of the game right in jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "Kd6Gi00D3JPF",
    "outputId": "8dc23a40-b7fd-4616-85e4-c53cff6e049b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD8CAYAAACINTRsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f82f0a6f588>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "screen = env.render(mode='rgb_array')\n",
    "plt.imshow(screen)\n",
    "ipythondisplay.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dn7BYEddyyfE"
   },
   "source": [
    "**Action space and observation space**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TC3Dim9_yyfG"
   },
   "source": [
    "Besides the pixels of the game, Gym actually gives you access to two important objects: actions that you can take, and observations (state of the game). Both actions and observations can be either discrete or continuous, depending on the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IAu1l8Z_yyfJ",
    "outputId": "15bdb5cd-69da-4795-f432-213116a53ba2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Box(1,), Box(2,))"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space, env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TnUaCXYmyyfX"
   },
   "source": [
    "Here, the action space is continuous: one number, ranging from +1 (go right) to -1 (go left).\n",
    "\n",
    "The observation space is also continuous: the position of the car (x) and its velocity (x')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HltKNdis3j57"
   },
   "source": [
    "**Interacting with the environment, getting rewards**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WxfGUoQ23nUs"
   },
   "source": [
    "Every Gym environment has a method step() that takes as argument the index of the action to take, and outputs the new state, the reward for taking this action, and tells us whether the game is done or not (here: if we reached the goal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mlQ4CCqXyyfZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S38AW8Fwyyfe"
   },
   "source": [
    "#### 1.3.2 FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rg3J--Mmyyfm",
    "outputId": "058e9530-e6b4-44cf-c297-2a2639e50809"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states:  Discrete(16)\n",
      "actions:  Discrete(4)\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Environment and Q-table structure\n",
    "env = gym.make('FrozenLake-v0')\n",
    "print('states: ', env.observation_space)\n",
    "print('actions: ', env.action_space)\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7XT0rCu8yyff"
   },
   "source": [
    "In this environment aim is to reach the goal G, on a frozen lake that might have some holes in it. Since the lake is frozen, **your actions are non deterministic**: you may try to go left and go down instead for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sSHAcIiKyyfg"
   },
   "source": [
    "S: starting point (safe), F: frozen surface (safe), H: hole (fall to your doom), G: goal (where the frisbee is located)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e6jVT5Heyyfi"
   },
   "source": [
    "- The environment is a grid of shape 4x4. Each state is a position on this grid (so 16 states), some are just frozen, some are holes, and one is the goal.\n",
    "- You always start in position 0 and have to go to position 15 (but you don't know it).\n",
    "- At each timestep you can go up, down, left, right (4 actions).\n",
    "- You get a reward 0 for anything except getting to position 15 which gives you reward 1.\n",
    "- The epsiode **ends** if you fall in a Hole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nm0I3ggLyyf7"
   },
   "source": [
    "Actions:\n",
    "- 0 = Left\n",
    "- 1 = Down\n",
    "- 2 = Right\n",
    "- 3 = Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yqsNlGl-yyf9"
   },
   "source": [
    "Try to go down. With a certain probability you actually go down, but sometimes you don't:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pdE5P2Ajyyf_",
    "outputId": "c83233ef-27e2-48a4-eadc-9ae27e8d2811"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env.step(1)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6zD7TLAfyygM"
   },
   "source": [
    "### 1.4 Solving FrozenLake by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vN6tN_6VyygO"
   },
   "source": [
    "Since we know the grid, we could cheat and implement a very simple algorithm that tries to reach G. We'd still not get to G all the time, because of the uncertainty of our movements.\n",
    "\n",
    "Yet, remmeber that our agent only knows his current position, but doesn't know which position is frozen and which is a hole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N1IgB-UZyygX"
   },
   "source": [
    "### 1.5 Solving FrozenLake with tabular Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6FYJ7BYlyygc"
   },
   "source": [
    "In tabular Q-learning, we learn the matrix Q(s,a) while interacting with the environment and use it to determine our policy.\n",
    "\n",
    "- Initialize matrix Q\n",
    "- At each timestep:\n",
    "    - Get current state s\n",
    "    - Use current matrix Q to get Q(s,a) for each a\n",
    "    - Choose the a with highest Q(s,a) (or take random a with prob. epsilon)\n",
    "    - Get new state s' and reward r from environment\n",
    "    - Use reward r to update matrix Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ye28HV79yygh"
   },
   "source": [
    "**TODO** clean up this code, train for longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set learning parameters\n",
    "eta = .628\n",
    "gamma = .9\n",
    "eps = 0.1\n",
    "num_episodes = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ulxCzI1yygs",
    "outputId": "f48daf5a-7b0e-4892-fb4e-08d591e93ed2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward Sum on all episodes: 51.80 %\n",
      "Final Values Q-Table\n",
      "[[6.79472670e-02 3.71884445e-03 3.24518641e-03 3.62626876e-03]\n",
      " [7.68147033e-04 1.66554703e-04 1.50711163e-03 1.35305916e-01]\n",
      " [1.37935910e-03 2.26995440e-03 1.08270518e-03 2.32836996e-02]\n",
      " [8.73282125e-04 1.10315675e-03 3.89838832e-04 1.33949310e-02]\n",
      " [1.04616168e-01 1.56567766e-03 3.55089270e-03 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.75172337e-04 1.98896690e-04 2.21516456e-02 4.39893845e-08]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.23780740e-03 6.69005442e-03 0.00000000e+00 1.67114194e-01]\n",
      " [0.00000000e+00 2.78720955e-01 7.70302172e-03 2.92921060e-03]\n",
      " [5.59274347e-01 0.00000000e+00 1.61159105e-03 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 6.10273890e-01 3.12902538e-03]\n",
      " [0.00000000e+00 0.00000000e+00 9.20939304e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# 3. Q-learning Algorithm\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "all_rewards = [] # total rewards per episode\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    # Reset environment\n",
    "    s = env.reset()\n",
    "    episode_reward = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    # The Q-Table learning algorithm\n",
    "    for j in range(100):\n",
    "        # Choose action from Q table (with eps-greedy exploration)\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        # Get new state & reward from environment\n",
    "        s1, r, done, _ = env.step(a)\n",
    "        # Update Q-Table with new knowledge\n",
    "        Q[s,a] = Q[s,a] + eta*(r + gamma*np.max(Q[s1,:]) - Q[s,a])\n",
    "        episode_reward += r\n",
    "        s = s1\n",
    "        if done:\n",
    "            break\n",
    "    all_rewards.append(episode_reward)\n",
    "    # env.render()\n",
    "print(\"Reward Sum on all episodes: %.2f %%\" % (100*np.mean(all_rewards),))\n",
    "print(\"Final Values Q-Table\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-uJ3gpTjyyhY",
    "outputId": "92954c4b-e248-480b-eed6-e44ed611de5e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUqMseElq1P8CQMzjyv7FjU0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1126876a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(Q, cmap='RdYlGn', aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "odhYikq8yyhm"
   },
   "source": [
    "**TODO** use the learned Q to do a few trials while printing the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DeH0gHuvyyho"
   },
   "source": [
    "## Part 2: Intro to TRFL with Approximate Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fRVKbPGcyyhp"
   },
   "source": [
    "### 2.1 What is TRFL and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1PvFO7XUyyhs"
   },
   "source": [
    "*TRFL implements a large group of DRL building blocks including state-value learning, distributional-value learning, continuous-action policy gradient, deterministic policy gradient and many others.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AvgoTOVjyyhv"
   },
   "source": [
    "**TODO** complete using Blog post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q9IqbtLPyyhz"
   },
   "source": [
    "### Value learning\n",
    "\n",
    "- State Value learning:\n",
    "**td_learning**\n",
    "generalized_lambda_returns\n",
    "td_lambda\n",
    "\n",
    "- Discrete-action Value learning:\n",
    "**qlearning**\n",
    "double_qlearning\n",
    "persistent_qlearning\n",
    "sarsa\n",
    "sarse\n",
    "qlambda\n",
    "\n",
    "- Distributional Value learning:\n",
    "categorical_dist_qlearning\n",
    "categorical_dist_double_qlearning\n",
    "categorical_dist_td_learning\n",
    "\n",
    "### Policy gradient\n",
    "- Continuous-action Policy Gradient:\n",
    "**policy_gradient**\n",
    "policy_gradient_loss\n",
    "policy_entropy_loss\n",
    "sequence_a2c_loss\n",
    "\n",
    "- Deterministic Policy Gradient:\n",
    "dpg\n",
    "\n",
    "- Discrete-action Policy Gradient:\n",
    "**discrete_policy_entropy_loss**\n",
    "sequence_advantage_actor_critic_loss: this is the commonly-used A2C/A3C loss function.\n",
    "discrete_policy_gradient\n",
    "discrete_policy_gradient_loss\n",
    "\n",
    "### ... and a few other things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oYlFlexRyyh1"
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NQAN_f4yyyh7"
   },
   "source": [
    "### 2.2 Installing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d8c6_Y2vyyh-"
   },
   "source": [
    "```pip install git+git://github.com/deepmind/trfl.git```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pY3vTtDnyyiE"
   },
   "source": [
    "And the requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pDyhz8nyyiI"
   },
   "source": [
    "```pip install tensorflow tensorflow-probability```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JMAgSTfryyiK"
   },
   "source": [
    "Note that you might still have some dependencies missing, for example `wrapt` was missing for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IeaRnXR9yyiM",
    "outputId": "01e7958f-8d50-461c-9ab1-46c36c5d72bd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeromekafrouni/.pyenv/versions/3.6.1/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/Users/jeromekafrouni/.pyenv/versions/3.6.1/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/Users/jeromekafrouni/.pyenv/versions/3.6.1/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  return _inspect.getargspec(target)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import trfl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pPAycvTeyyiW"
   },
   "source": [
    "### 2.3 Back to FrozenLake, with Approximate Q-Learning (and DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RAIWRof1yyiZ"
   },
   "source": [
    "We still want to solve the same problem, so the states and actions are the same.\n",
    "\n",
    "Our goal also remains the same: learning the action value function Q(s,a) while interacting with the environment. Instead of learning the whole Q matrix (of shape (num_states,num_actions)), we want to learn an **approximate function (represented by a neural network) that takes as input (s,a) and outputs an approximation of the solution to the Bellman equation**.\n",
    "\n",
    "As before, we will still choose an action a by taking the argmax of Q(s,a) for a given s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FySnzMtvyyib"
   },
   "source": [
    "Since the code gets a bit more complex, we introduce a class Agent for cleaner and more reusable code. Note that the way we interact with the environment stays very similar, only the training changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5fKpw6-Myyie"
   },
   "outputs": [],
   "source": [
    "def one_hot(n_classes, i):\n",
    "    return np.eye(n_classes)[i].reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FjVQJxnKyyim"
   },
   "outputs": [],
   "source": [
    "# Set learning parameters\n",
    "gamma = .99\n",
    "eps = 0.1\n",
    "num_episodes = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eWfiS9L2yyi3"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    # TODO implement batches\n",
    "    def __init__(self, env, discount, eps, lr):\n",
    "        self.env = env\n",
    "        self.eps = eps # epsilon-greedy\n",
    "        self.sess = tf.Session()\n",
    "        self.n_states = env.observation_space.n\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.make_model(discount, lr)\n",
    "        \n",
    "    def make_model(self, discount, lr):\n",
    "        self.state = tf.placeholder(shape=[1, self.n_states], dtype=tf.float32, name='state') # one hot encoding\n",
    "        # W = tf.Variable(tf.random_uniform([self.n_states, self.n_actions], 0, 0.01))\n",
    "        # self.Q = tf.matmul(self.state, W)\n",
    "        dense = tf.layers.dense(self.state, 64, activation='relu')\n",
    "        self.Q = tf.layers.dense(dense, self.n_actions, activation='linear')\n",
    "        \n",
    "        self.action = tf.placeholder(dtype=tf.int32, name='action')\n",
    "        self.reward = tf.placeholder(dtype=tf.float32, name='reward')\n",
    "        self.discount = tf.constant([discount])\n",
    "\n",
    "        self.Q_target = tf.placeholder(dtype=tf.float32)\n",
    "        loss, q_learning = trfl.qlearning(self.Q, self.action, self.reward, self.discount, self.Q_target)\n",
    "        reduced_loss = tf.reduce_mean(loss)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "        self.train_op = optimizer.minimize(reduced_loss)\n",
    "    \n",
    "    def predict(self, s):\n",
    "        # TODO maybe launch session here instead of in train()\n",
    "        return self.sess.run(self.Q, feed_dict={self.state: one_hot(self.n_states, s)})\n",
    "    \n",
    "    def act(self, s):\n",
    "        Q = self.predict(s)\n",
    "        if np.random.rand(1) < self.eps:\n",
    "            a = [env.action_space.sample()]\n",
    "        else:\n",
    "            a = np.argmax(Q, 1)\n",
    "        return a\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        episode_rewards = []\n",
    "        with self.sess:\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            for i in range(num_episodes):\n",
    "                episode_reward = 0\n",
    "                # Reset environment and get first new observation\n",
    "                s = self.env.reset()\n",
    "                # The Q-Network\n",
    "                for j in range(100):\n",
    "                    a = self.act(s)\n",
    "                    s1, r, done, _ = self.env.step(a[0])\n",
    "                    episode_reward += r\n",
    "                    if done and r != 1: # done early with no rewards means falling in a hole\n",
    "                        r = -1\n",
    "                    Q_target = self.predict(s1)\n",
    "                    self.sess.run(self.train_op,\n",
    "                                  {self.state: one_hot(self.n_states, s),\n",
    "                                   self.Q_target: Q_target,\n",
    "                                   self.action: a,\n",
    "                                   self.reward: r})\n",
    "                    s = s1\n",
    "                    if done:\n",
    "                        # TODO take care of update of eps if we call train multiple times?\n",
    "                        # TODO discount eps differently\n",
    "                        # Reduce chance of random action as we train the model.\n",
    "                        # self.eps = 1./((i/50) + 10)\n",
    "                        break\n",
    "                episode_rewards.append(episode_reward)\n",
    "                if i % 1000 == 0:\n",
    "                    print('Episode %s done, average episode reward: %.2f %%' % (i, 100*np.mean(episode_rewards[-1000:]),))\n",
    "        print(\"Percent of succesful episodes: %.2f %%\" % (100*np.mean(episode_rewards),))\n",
    "        return episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rcFBNTlVyyjN"
   },
   "source": [
    "**TODO train longer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P8odGm4wyyjY",
    "outputId": "640e0a86-06a6-491f-dc2b-698e0a80c1cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:indexing function cannot get shapes for tensors \"values\" and \"indices\" at construction time, and so can't check that their shapes are valid or compatible. Incorrect indexing may occur at runtime without error!\n",
      "Episode 0 done, average episode reward: 0.00 %\n",
      "Episode 1000 done, average episode reward: 23.30 %\n",
      "Percent of succesful episodes: 28.90 %\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(env, discount=0.99, eps=0.1, lr=0.1)\n",
    "rewards = agent.train(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bb8ONQSVyyjf"
   },
   "source": [
    "#### Switching from regular Q-learning to Sarsa:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dljFrZ5hyyjf"
   },
   "source": [
    "Thanks to trfl, we can change our learning algorithm by only changing a few lines in the code. For example, we can use Sarsa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mrU3pzQFyyjh"
   },
   "outputs": [],
   "source": [
    "class SarsaAgent:\n",
    "    # TODO implement batches\n",
    "    def __init__(self, env, discount, eps, lr):\n",
    "        self.env = env\n",
    "        self.eps = eps # epsilon-greedy\n",
    "        self.sess = tf.Session()\n",
    "        self.n_states = env.observation_space.n\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.make_model(discount, lr)\n",
    "        \n",
    "    def make_model(self, discount, lr):\n",
    "        self.state = tf.placeholder(shape=[1, self.n_states], dtype=tf.float32, name='state') # one hot encoding\n",
    "        # W = tf.Variable(tf.random_uniform([self.n_states, self.n_actions], 0, 0.01))\n",
    "        # self.Q = tf.matmul(self.state, W)\n",
    "        dense = tf.layers.dense(self.state, 64, activation='relu')\n",
    "        self.Q = tf.layers.dense(dense, self.n_actions, activation='linear')\n",
    "        \n",
    "        self.action = tf.placeholder(dtype=tf.int32, name='action')\n",
    "        self.reward = tf.placeholder(dtype=tf.float32, name='reward')\n",
    "        self.discount = tf.constant([discount])\n",
    "\n",
    "        self.Q_target = tf.placeholder(dtype=tf.float32)\n",
    "        self.action_target = tf.placeholder(dtype=tf.int32)\n",
    "        loss, q_learning = trfl.sarsa(\n",
    "            self.Q, self.action, self.reward, self.discount, self.Q_target, self.action_target)\n",
    "        reduced_loss = tf.reduce_mean(loss)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "        # this will update only W (because of how qlearning is implemented):\n",
    "        self.train_op = optimizer.minimize(reduced_loss)\n",
    "    \n",
    "    def predict(self, s):\n",
    "        # TODO maybe launch session here instead of in train()\n",
    "        return self.sess.run(self.Q, feed_dict={self.state: one_hot(self.n_states, s)})\n",
    "    \n",
    "    def act(self, s):\n",
    "        Q = self.predict(s)\n",
    "        if np.random.rand(1) < self.eps:\n",
    "            a = [env.action_space.sample()]\n",
    "        else:\n",
    "            a = np.argmax(Q, 1)\n",
    "        return a\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        episode_rewards = []\n",
    "        with self.sess:\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            for i in range(num_episodes):\n",
    "                episode_reward = 0\n",
    "                # Reset environment and get first new observation\n",
    "                s = self.env.reset()\n",
    "                # The Q-Network\n",
    "                for j in range(100):\n",
    "                    a = self.act(s)\n",
    "                    s1, r, done, _ = self.env.step(a[0])\n",
    "                    episode_reward += r\n",
    "                    if done and r != 1: # done early with no rewards means falling in a hole\n",
    "                        r = -1\n",
    "                    Q_target = self.predict(s1)\n",
    "                    action_target = self.act(s1)\n",
    "                    self.sess.run(self.train_op,\n",
    "                                  {self.state: one_hot(self.n_states, s),\n",
    "                                   self.Q_target: Q_target,\n",
    "                                   self.action: a,\n",
    "                                   self.action_target: action_target,\n",
    "                                   self.reward: r})\n",
    "                    s = s1\n",
    "                    if done:\n",
    "                        # TODO take care of update of eps if we call train multiple times?\n",
    "                        # TODO discount eps differently\n",
    "                        # Reduce chance of random action as we train the model.\n",
    "                        # self.eps = 1./((i/50) + 10)\n",
    "                        break\n",
    "                episode_rewards.append(episode_reward)\n",
    "                if i % 1000 == 0:\n",
    "                    print('Episode %s done, average episode reward: %.2f %%' % (i, 100*np.mean(episode_rewards[-1000:]),))\n",
    "        print(\"Percent of succesful episodes: %.2f %%\" % (100*np.mean(episode_rewards),))\n",
    "        return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u9MMBr1uyyjs",
    "outputId": "55d3a9c3-6e62-45c3-b15d-f7233a9a3ff0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:indexing function cannot get shapes for tensors \"values\" and \"indices\" at construction time, and so can't check that their shapes are valid or compatible. Incorrect indexing may occur at runtime without error!\n",
      "WARNING:tensorflow:indexing function cannot get shapes for tensors \"values\" and \"indices\" at construction time, and so can't check that their shapes are valid or compatible. Incorrect indexing may occur at runtime without error!\n",
      "Episode 0 done, average episode reward: 0.00 %\n",
      "Episode 1000 done, average episode reward: 6.20 %\n",
      "Percent of succesful episodes: 17.10 %\n"
     ]
    }
   ],
   "source": [
    "sarsa_agent = SarsaAgent(env, discount=0.99, eps=0.1, lr=0.1)\n",
    "rewards = sarsa_agent.train(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BbpP3O3nyyj3"
   },
   "source": [
    "### Part 3: Going further, solving harder games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bN21BS_Jyyj6"
   },
   "source": [
    "Above, we used approximate Q-learning on a game that didn't require it (because we could actually store and learn the full Q matrix). Now that we learned how to do that, we want to use Deep RL for when we actually need it: when the state and/or space action is too large to do tabular Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a9vZC2N6yyj9"
   },
   "source": [
    "Let's choose a game a bit harder than FrozenLake but on which we can still easily train. What we'll do can extend easily to harder games, such as Atari games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N0og015Hyyj-"
   },
   "source": [
    "**TODO** complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O5WsR1YSyykB",
    "outputId": "37641476-94d9-42b4-8b24-6fa5bd47c8fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeromekafrouni/.pyenv/versions/3.6.1/Python.framework/Versions/3.6/lib/python3.6/site-packages/gym/envs/registration.py:14: DeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Discrete(2), Box(4,))"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.action_space, env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lYrn8Ee4yykG"
   },
   "source": [
    "**TODO** print inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BLFzWiFOyykM"
   },
   "source": [
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TxqYX2cgyykO"
   },
   "source": [
    "There are only two actions: pushing left or right (+1 or -1), but the state space is now continuous, described by 4 variables: x, x', theta, theta'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XZ611o3dyykO"
   },
   "source": [
    "We would easily adapt our previous agent's architecture for this task: the only thing to change is how the states are encoded (used to be one-hot encoded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2JJDgV8yykU"
   },
   "outputs": [],
   "source": [
    "# TODO maybe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tOrLQyMAyykd"
   },
   "source": [
    "Let's use policy-gradient methods instead to learn how to use them in TRFL:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BTUOG7ncyykf"
   },
   "source": [
    "Now we need to use the current policy over one episode, collect all the rewards, the over that replay compute the values of each step (now that we know the horizon). And then pass it to trfl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1I0_nGatyykg"
   },
   "outputs": [],
   "source": [
    "class PolicyGradientAgent:\n",
    "    # TODO implement batches\n",
    "    def __init__(self, env, discount, eps, lr):\n",
    "        self.env = env\n",
    "        self.eps = eps # epsilon-greedy\n",
    "        self.sess = tf.Session()\n",
    "        self.dim_state = env.observation_space.shape[0]\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.discount = discount\n",
    "        self.make_model(lr)\n",
    "        \n",
    "    def make_model(self, lr):\n",
    "        self.state = tf.placeholder(shape=[1, self.dim_state], dtype=tf.float32, name='state')\n",
    "        dense = tf.layers.dense(self.state, 50, activation='relu')\n",
    "        # dense = tf.layers.dense(dense, 12, activation='relu')\n",
    "        self.policy = tf.layers.dense(dense, self.n_actions, activation='softmax')\n",
    "        \n",
    "        # self.replay_states = tf.placeholder(dtype=tf.float32, name='replay_states')\n",
    "        self.replay_action = tf.placeholder(dtype=tf.int32, name='replay_action')\n",
    "        self.replay_value = tf.placeholder(dtype=tf.float32, name='replay_value')\n",
    "        \n",
    "        loss = trfl.discrete_policy_gradient(self.policy, # TODO try to do the whole batch at once\n",
    "                                             self.replay_action,\n",
    "                                             self.replay_value)\n",
    "        reduced_loss = tf.reduce_mean(loss)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        self.train_op = optimizer.minimize(reduced_loss)\n",
    "    \n",
    "    def predict(self, s):\n",
    "        # TODO maybe launch session here instead of in train()\n",
    "        return self.sess.run(self.policy, feed_dict={self.state: s.reshape(1,-1)})\n",
    "    \n",
    "    def act(self, s):\n",
    "        p = self.predict(s)\n",
    "        p = p/p.sum() # trick to normalize p for very small precision issues\n",
    "        if np.random.rand(1) < self.eps:\n",
    "            a = [env.action_space.sample()]\n",
    "        else:\n",
    "            # a = np.argmax(p, 1)\n",
    "            a = [np.random.choice(range(p.shape[1]), p=p.ravel())]\n",
    "        return a\n",
    "    \n",
    "    def compute_values(self, rewards):\n",
    "        \"\"\"\n",
    "        Given all the rewards received during one episode, compute the value\n",
    "        of each action.\n",
    "        \"\"\"\n",
    "        values = [0] * len(rewards)\n",
    "        rewards.reverse()\n",
    "        for i, r in enumerate(rewards):\n",
    "            values[i] = r + self.discount * values[i-1]\n",
    "        values.reverse()\n",
    "        return values\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        episode_rewards = []\n",
    "        with self.sess:\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            for i in range(num_episodes):\n",
    "                states, actions, rewards = [], [], []\n",
    "                # Reset environment and get first new observation\n",
    "                s = self.env.reset()\n",
    "                # The Q-Network\n",
    "                done = False\n",
    "                while not done:\n",
    "                    # we don't train at each timestep anymore,\n",
    "                    # we use the same policy throughout one episode\n",
    "                    # and then use the 'replay' to learn\n",
    "                    a = self.act(s)\n",
    "                    s1, r, done, _ = self.env.step(a[0])\n",
    "                    # store replay:\n",
    "                    states.append(s)\n",
    "                    actions.append(a)\n",
    "                    rewards.append(r)\n",
    "                    \n",
    "                    s = s1\n",
    "                # now use the replay of that episode to train:\n",
    "                values = self.compute_values(rewards)\n",
    "                for s, a, v in zip(states, actions, values):\n",
    "                    # TODO try to do the whole batch at once\n",
    "                    self.sess.run(self.train_op,\n",
    "                                  {self.state: s.reshape(1,-1),\n",
    "                                   self.replay_action: a,\n",
    "                                   self.replay_value: np.array([v])})\n",
    "                episode_rewards.append(sum(rewards))\n",
    "                if i % 100 == 0:\n",
    "                    print(\"Episode %s, Average reward per episode: %.2f\" % (i, np.mean(episode_rewards[-100:]),))\n",
    "                    print(self.predict(s))\n",
    "        return episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0OXdjT_uyykl",
    "outputId": "232b752d-f12f-4ed7-fbdd-6734294c2ff0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Average reward per episode: 15.00\n",
      "[[0.52634764 0.47365236]]\n",
      "Episode 100, Average reward per episode: 46.45\n",
      "[[0.9919084  0.00809162]]\n",
      "Episode 200, Average reward per episode: 82.35\n",
      "[[0.01836267 0.9816373 ]]\n",
      "Episode 300, Average reward per episode: 105.01\n",
      "[[1.000000e+00 8.861978e-12]]\n",
      "Episode 400, Average reward per episode: 97.84\n",
      "[[4.436048e-10 1.000000e+00]]\n",
      "Episode 500, Average reward per episode: 78.94\n",
      "[[1.0970825e-09 1.0000000e+00]]\n",
      "Episode 600, Average reward per episode: 65.18\n",
      "[[3.073839e-04 9.996927e-01]]\n",
      "Episode 700, Average reward per episode: 66.55\n",
      "[[9.999999e-01 7.442094e-08]]\n",
      "Episode 800, Average reward per episode: 65.33\n",
      "[[5.262675e-10 1.000000e+00]]\n",
      "Episode 900, Average reward per episode: 70.74\n",
      "[[1.7775509e-09 1.0000000e+00]]\n",
      "Episode 1000, Average reward per episode: 74.59\n",
      "[[3.1439915e-10 1.0000000e+00]]\n",
      "Episode 1100, Average reward per episode: 68.75\n",
      "[[7.0573833e-06 9.9999297e-01]]\n",
      "Episode 1200, Average reward per episode: 75.74\n",
      "[[1.8587718e-07 9.9999976e-01]]\n",
      "Episode 1300, Average reward per episode: 70.85\n",
      "[[1.0000000e+00 8.8226955e-20]]\n",
      "Episode 1400, Average reward per episode: 80.54\n",
      "[[5.005315e-15 1.000000e+00]]\n",
      "Episode 1500, Average reward per episode: 96.09\n",
      "[[1.3332258e-15 1.0000000e+00]]\n",
      "Episode 1600, Average reward per episode: 117.44\n",
      "[[7.911655e-14 1.000000e+00]]\n",
      "Episode 1700, Average reward per episode: 99.08\n",
      "[[1.2659217e-10 1.0000000e+00]]\n",
      "Episode 1800, Average reward per episode: 113.18\n",
      "[[1. 0.]]\n",
      "Episode 1900, Average reward per episode: 98.09\n",
      "[[3.2456933e-17 1.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "policy_agent = PolicyGradientAgent(env, discount=0.99, eps=0.1, lr=0.001)\n",
    "rewards = policy_agent.train(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RcxXxsAnyyk0"
   },
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "S38AW8Fwyyfe",
    "bb8ONQSVyyjf"
   ],
   "name": "trfl-tutorial.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
